{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on EMMIT framework\n",
    "\n",
    "This notebook shows how to use the `EMMIT` framework that explains a deep learning model behaviour. \n",
    "\n",
    "The model that we will use for this example is a CNN trained on the data from the [Ballroom](http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html) dataset. The dataset contains audio recordings of 8 classical dances: Samba, Jive, Rumba, Quickstep, Tango, Cha Cha, VienneseWaltz and Waltz. For more details about architecture, training procedure of the model, please check folder `models`. Here we demonstrate how to use two pipelines:\n",
    "\n",
    "1. The pipeline that transforms audio files based on the configuration file.\n",
    "2. The pipeline that generates confusion matrices based on model predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation pipeline\n",
    "\n",
    "The first step is creating a configuration file (an example is in `configuration.yml`) that describes which transformations and to which extent need to be applied to the data. Then, a target classification model will be run on all of the transformed files. In our case, predictions are the type of a dance. A YAML file will be generated as a result, where information about transformations and information about the predictions is stored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configuration for audio augmentation pipeline\n",
    "\n",
    "Here are the parameters that need to be defined in a configuration file:\n",
    "\n",
    "- `augmented_audio_save_path`: The path to save the augmented audio outputs.\n",
    "- `augmented_meta_save_path`: The path to save the augmented metadata.\n",
    "- `mir_dataset_path`: The path to save the data sources.\n",
    "- `hpss`: Set up harmonic/percussive source separation.\n",
    "- `tempo_factor`: Set up logspace time stretch, for the detail of parameter, please check [muda](https://muda.readthedocs.io/en/stable/#).\n",
    "- `keys`: Set up linear pitch shift, for the detail of parameter, please check [muda](https://muda.readthedocs.io/en/stable/#).\n",
    "- `drc`: Set up dynamic range compression, for the detail of parameter, please check [muda](https://muda.readthedocs.io/en/stable/#).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A safe way to install the library would be to create a virtual environment:\n",
    "\n",
    "```\n",
    "\n",
    "conda create --name emmit_env python=3.11\n",
    "conda activate emmit_env\n",
    "pip install -r requirements.txt\n",
    "python -m ipykernel install --user --name emmit_env\n",
    "\n",
    "```me musicnn_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.float_ = np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emmit import aitah\n",
    "\n",
    "# Create an instance of AudioTransformation from the aitah module, configured via 'configuration.yml'\n",
    "audio_bank = aitah.AudioTransformation(config_file=\"configuration.yml\")\n",
    "# Generate augmented audio samples based on the specified number of samples in the configuration\n",
    "audio_bank.synthesis(n_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding augmented audio samples to the model\n",
    "\n",
    "Let's load a model that we will be using in this tutorial and save prediction results. Replace with your own model to get your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained model from the directory specified\n",
    "model = tf.saved_model.load(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAV_PATH = r\"metadata_with_predict_results.csv\"\n",
    "\n",
    "if os.path.exists(SAV_PATH):\n",
    "    sav_df = pd.read_csv(SAV_PATH)\n",
    "    # Get the list of filenames from sav_df\n",
    "    existing_filenames = sav_df[\"file_name\"].tolist()\n",
    "\n",
    "    # Filter out rows in int_df that contain filenames in the existing_filenames list\n",
    "    int_df = int_df[~int_df[\"file_name\"].isin(existing_filenames)]\n",
    "    int_df.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    print(f\"File {SAV_PATH} does not exist.\")\n",
    "\n",
    "\n",
    "for i, row in tqdm(int_df.iterrows(), total=int_df.shape[0]):\n",
    "\n",
    "    # Join the path with the file name for each row in the DataFrame\n",
    "    audio_file = os.path.join(\n",
    "        config[\"augmented_audio_save_path\"], row[\"type\"], row[\"file_name\"] + \".wav\"\n",
    "    )\n",
    "    prediction = model(audio_file) #you might need to replace this part depending on how your model's inference is triggered\n",
    "\n",
    "    int_df.at[i, \"y\"] = prediction[\"class_names\"].numpy().decode(\"utf-8\")\n",
    "    int_df.at[i, \"y_id\"] = prediction[\"class_ids\"].numpy()\n",
    "    # Get row i from the DataFrame\n",
    "    current_row = pd.DataFrame(int_df.iloc[i]).T\n",
    "    current_row.to_csv(SAV_PATH, mode=\"a\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictions with transformations metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"metadata_with_predict_results_tempo_only.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation pipeline\n",
    "\n",
    "The interpretation pipeline of EMMIT presents techniques for understanding the outcomes of how the model reacts to modified files, including **accuracy impact summary table**, **confusion matrix**, and **LIME plot**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting confusion matrix\n",
    "\n",
    "You can plot the confusion matrix by following function:\n",
    "\n",
    "- `visualize_in_notebook`: Plot the confusion matrix in the notebook.\n",
    "- `save_as_file`: Save the confusion matrix as a file.\n",
    "- `visualize_subtracted_in_notebook`: Plot the confusion matrix with the target confusion matrix subtracted from it.\n",
    "- `save_subtracted_as_file`: Save the confusion matrix with the target confusion matrix subtracted from it as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emmt import palun\n",
    "\n",
    "# Create an instance of the ConfusionMatrix class from the palun module\n",
    "palun = palun.ConfusionMatrix()\n",
    "filtered_df = df[df[\"rate\"] == 0.7071067811865476]\n",
    "\n",
    "\n",
    "# Use the visualize_in_notebook method to display the confusion matrix in the notebook\n",
    "# The method takes two arguments: the true labels and the predicted labels\n",
    "palun.visualize_in_notebook(filtered_df[\"type\"], filtered_df[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code saves the confusion matrix as a PNG file named \"confusion_matrix.png\".\n",
    "# It uses the 'type' column from the dataframe 'df' as the class labels and the 'y' column as the predictions.\n",
    "palun.save_as_file(\n",
    "    music_class=df[\"type\"], prediction=df[\"y\"], filename=\"confusion_matrix.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Here you can input a confusion matrix produced by the model originally on the same data that was input into \n",
    "# EMMIT framework\n",
    "target_confusion_matrix = np.array(\n",
    "    [\n",
    "        [42, 0, 3, 0, 1, 3, 0, 0],  # True ChaChaCha classified as each dance type\n",
    "        [2, 12, 4, 1, 1, 0, 2, 0],  # True Jive classified as each dance type\n",
    "        [1, 0, 30, 3, 0, 0, 2, 0],  # True Quickstep classified as each dance type\n",
    "        [0, 1, 5, 30, 0, 0, 5, 4],  # True Rumba classified as each dance type\n",
    "        [0, 2, 4, 0, 30, 2, 0, 0],  # True Samba classified as each dance type\n",
    "        [2, 0, 1, 1, 0, 27, 3, 0],  # True Tango classified as each dance type\n",
    "        [0, 0, 0, 4, 0, 0, 26, 0],  # True VienneseWaltz classified as each dance type\n",
    "        [0, 0, 1, 2, 0, 0, 10, 35],  # True Waltz classified as each dance type\n",
    "    ]\n",
    ")\n",
    "\n",
    "# List of class names corresponding to the dances in the confusion matrix.\n",
    "classes = [\n",
    "    \"ChaChaCha\",\n",
    "    \"Jive\",\n",
    "    \"Quickstep\",\n",
    "    \"Rumba\",\n",
    "    \"Samba\",\n",
    "    \"Tango\",\n",
    "    \"VienneseWaltz\",\n",
    "    \"Waltz\",\n",
    "]\n",
    "\n",
    "# Visualize the difference between the target confusion matrix and the actual confusion matrix\n",
    "# derived from the 'type' and 'y' columns of the dataframe 'df'. This visualization helps in\n",
    "# understanding how the predicted classifications deviate from the expected ones.\n",
    "palun.visualize_subtracted_in_notebook(\n",
    "    target_classes=classes,\n",
    "    target_confusion_matrix=target_confusion_matrix,\n",
    "    music_class=filtered_df[\"type\"],\n",
    "    prediction=filtered_df[\"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting LIME explanation\n",
    "\n",
    "`EMMT` also implements the LIME technique for interpreting the MIR task model. The method centers on training Logistic Regression with Lasso regularization on transformed audio attributes, to estimate the forecasts of the CNN model underneath. Its objective is to comprehend the reasoning behind a specific prediction made by the CNN model, with the explanatory model reducing loss and complexity while closely imitating the original modelâ€™s forecasts.\n",
    "\n",
    "You can plot LIME explanation by following function:\n",
    "\n",
    "- `show_lime_explanation`: Plot the LIME explanation in the notebook.\n",
    "- `save_lime_as_file`: Save the LIME explanation as a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"metadata_with_predict_results_all.csv\")\n",
    "df[\"preset_id\"] = le.fit_transform(df[\"preset\"])\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emmt import palun\n",
    "\n",
    "lime_explanation = palun.LimeExplainer()\n",
    "features = [\"n_semitones\", \"rate\", \"hpss\", \"preset_id\"]\n",
    "instance = df.iloc[0][features]\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explanation.show_lime_explanation(\n",
    "    local_data=df[features],\n",
    "    predictions=df[\"y_id\"],\n",
    "    instance=instance,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explanation.save_lime_as_file(\n",
    "    filename=\"lime_explanation.png\",\n",
    "    local_data=df[features],\n",
    "    predictions=df[\"y_id\"],\n",
    "    instance=instance,\n",
    "    features=features,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emmit_env",
   "language": "python",
   "name": "emmit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
